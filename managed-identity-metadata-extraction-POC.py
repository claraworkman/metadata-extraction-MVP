"""
Direct Metadata Extraction from Original Contracts
Extracts metadata from Swedish, Polish, Estonian, etc. contracts without translation
Supports .txt, .docx, and .pdf files
Outputs to CSV for easy download and Sirion import
"""

import os
import os.path
import json
import csv
from pathlib import Path
from dotenv import load_dotenv
from openai import AzureOpenAI
from azure.identity import DefaultAzureCredential, get_bearer_token_provider
from datetime import datetime
import time
from docx import Document
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.storage.blob import BlobServiceClient
import io
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

load_dotenv()

# Azure OpenAI Configuration with Azure AD authentication
credential = DefaultAzureCredential()
token_provider = get_bearer_token_provider(
    credential, 
    "https://cognitiveservices.azure.com/.default"
)

client = AzureOpenAI(
    azure_ad_token_provider=token_provider,
    api_version="2025-01-01-preview",
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
)

# Azure Document Intelligence client for PDF OCR
document_intelligence_endpoint = os.getenv("DOCUMENT_INTELLIGENCE_ENDPOINT")
if document_intelligence_endpoint:
    doc_client = DocumentAnalysisClient(
        endpoint=document_intelligence_endpoint,
        credential=credential
    )
else:
    doc_client = None

# Azure Blob Storage client
storage_account_name = os.getenv("STORAGE_ACCOUNT_NAME")
if storage_account_name:
    blob_service_client = BlobServiceClient(
        account_url=f"https://{storage_account_name}.blob.core.windows.net",
        credential=credential
    )
else:
    blob_service_client = None

DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")

# Markdown output configuration
MARKDOWN_CONTAINER = os.getenv("MARKDOWN_CONTAINER", "extracted-markdown")  # Container for OCR markdown files
SAVE_MARKDOWN = os.getenv("SAVE_MARKDOWN", "true").lower() == "true"  # Save extracted text as markdown

# Accuracy configuration
USE_TWO_CALL_FOR_PDFS = os.getenv("USE_TWO_CALL_FOR_PDFS", "true").lower() == "true"  # Always translate PDFs first
ALWAYS_USE_TWO_CALL = os.getenv("ALWAYS_USE_TWO_CALL", "false").lower() == "true"  # Force translation for all files

# Parallel processing configuration
MAX_WORKERS = int(os.getenv("MAX_WORKERS", "10"))  # Concurrent threads
MAX_RETRIES = int(os.getenv("MAX_RETRIES", "3"))   # Retry attempts per file
RETRY_DELAY = float(os.getenv("RETRY_DELAY", "2")) # Seconds between retries

# Thread-safe progress tracking
class ProgressTracker:
    def __init__(self, total):
        self.total = total
        self.completed = 0
        self.lock = threading.Lock()
    
    def increment(self):
        with self.lock:
            self.completed += 1
            return self.completed

# Sirion metadata fields
SIRION_FIELDS = [
    "Original File Name",
    "Counterparty Legal Entity Name",
    "Internal Contracting Entity",
    "Contract Type",
    "Term Type",
    "Effective Date",
    "Expiration Date",
    "Governing Law",
    "Payment Term",
    "Contract Name",
    "Scope Category level 1",
    "Related Master Agreement"
]

SYSTEM_PROMPT = """You are a multilingual contract metadata extraction specialist. You can read contracts in Swedish, Norwegian, Danish, Polish, Latvian, Lithuanian, and Estonian.

Extract the following fields from contract documents. READ the contract in its original language, but RETURN all field values in ENGLISH:

Required fields (return values in ENGLISH):
1. Original File Name - The complete filename with extension (e.g., "supply_agreement.pdf")
2. Counterparty Legal Entity Name - The supplier/vendor company name (keep original)
3. Internal Contracting Entity - The Circle K entity name (keep original)
4. Contract Type - MUST be one of these exact options: "Master Agreement", "Product/ Service Specific Agreement", "Non Disclosure Agreement", "Statement of Work", "Amendment/ Addendum", "Change Order", "Order Form", "Supporting Document"
5. Term Type - Must be one of: "Perpetual", "Fixed", "Auto-Renewal"
6. Effective Date - Contract start date (MM/DD/YYYY format)
7. Expiration Date - Contract end date (MM/DD/YYYY format). IMPORTANT: Only populate this field if Term Type is "Fixed" or "Auto-Renewal". Leave null for "Perpetual" contracts.
8. Governing Law - Jurisdiction only (e.g., "Poland", "Norway", "Sweden", "Estonia")
9. Payment Term - Payment terms in days format (e.g., "Net 30", "Net 21", "Net 60")
10. Contract Name - MUST follow this format: {Contract Type}_{Supplier Name}_{Effective Date}. Example: "Supply Agreement_Scandinavian Food Suppliers AB_04/01/2024"
11. Scope Category level 1 - Choose ONLY from: "Technology", "Real Estate & Construction", "Operations and Logistics". Analyze the contract scope and select the most appropriate category.
12. Related Master Agreement - For documents that relate to a parent/master agreement, use this exact format: "{This Document Type} to: {Parent Agreement Name/Identifier}". Examples:
   - "Amendment to: Master Supply Agreement_04/01/2024"
   - "NDA related to: Master Agreement_TechServices O√ú_04/01/2024"
   - "SOW under: Master Services Agreement_Polski Dostawca_01/15/2025"
   - "Order Form for: Master Agreement dated 04/01/2024"
   Look for references like "amends agreement dated", "pursuant to master agreement", "reference to agreement", agreement numbers, or parent contract identifiers. Return null ONLY if this is a standalone Master Agreement or Product/Service Agreement with no parent contract referenced.

IMPORTANT - HANDLING CONTRACTS WITH ADDENDUMS:
- If the document contains both a main contract and addendums (amendments/annexes):
  * Effective Date: Use the start date from the MAIN/ORIGINAL contract
  * Expiration Date: Use the LATEST end date from any addendum or amendment (this supersedes the original contract end date)
  * Review ALL pages including addendums to find the most recent expiration date
  * If document is an addendum, Contract Type should be "Amendment/ Addendum"
- Addendums typically extend or modify the original contract terms
- Look for phrases like "extended until", "amended to expire", "new end date", "prolongation until"

DATE FORMAT RULES:
- Always use MM/DD/YYYY format (e.g., 04/01/2024, not 2024-04-01)
- Contract Name must use the same date format: Contract Type_Supplier_MM/DD/YYYY

TRANSLATION GUIDANCE FOR CONTRACT TYPES:
- Supply agreements, service agreements, distribution agreements ‚Üí "Product/ Service Specific Agreement"
- Framework agreements, umbrella agreements ‚Üí "Master Agreement"
- Confidentiality agreements, secrecy agreements ‚Üí "Non Disclosure Agreement"
- Work orders, service orders, project statements ‚Üí "Statement of Work"
- Amendments, annexes, supplements, addendums ‚Üí "Amendment/ Addendum"
- Modification orders, variation orders ‚Üí "Change Order"
- Purchase orders, sales orders ‚Üí "Order Form"
- Attachments, exhibits, appendices, schedules ‚Üí "Supporting Document"

SCOPE CATEGORY GUIDANCE:
- "Technology": IT services, software, hardware, telecommunications
- "Real Estate & Construction": Property leases, construction services, facility management
- "Operations and Logistics": Supply agreements, distribution, transportation, warehousing, food supplies

CRITICAL: NEVER leave Internal Contracting Entity or Counterparty Legal Entity Name empty. Search the entire contract for company names.

FEW-SHOT EXAMPLES:

Example 1 (Swedish supply agreement - perpetual):
Input: "Circle K Sverige AB... Scandinavian Food Suppliers AB... leveransavtal... tr√§der i kraft den 1 april 2024... l√∂per p√• obest√§md tid... livsmedel och dryck..."
Output: {"Original File Name": "swedish_supply_agreement.pdf", "Counterparty Legal Entity Name": "Scandinavian Food Suppliers AB", "Internal Contracting Entity": "Circle K Sverige AB", "Contract Type": "Product/ Service Specific Agreement", "Term Type": "Perpetual", "Effective Date": "04/01/2024", "Expiration Date": null, "Governing Law": "Sweden", "Payment Term": "Net 60", "Contract Name": "Product/ Service Specific Agreement_Scandinavian Food Suppliers AB_04/01/2024", "Scope Category level 1": "Operations and Logistics", "source_language": "sv", "confidence": "high"}

Example 2 (Polish supply agreement - auto-renewal):
Input: "Circle K Polska Sp. z o.o.... Polski Dostawca ≈ªywno≈õci Sp. z o.o.... umowa dostawy... wchodzi w ≈ºycie 1 kwietnia 2024... obowiƒÖzuje przez okres 24 miesiƒôcy do 31 marca 2026... automatycznie przed≈Çu≈ºa siƒô..."
Output: {"Original File Name": "polish_supply_agreement.pdf", "Counterparty Legal Entity Name": "Polski Dostawca ≈ªywno≈õci Sp. z o.o.", "Internal Contracting Entity": "Circle K Polska Sp. z o.o.", "Contract Type": "Product/ Service Specific Agreement", "Term Type": "Auto-Renewal", "Effective Date": "04/01/2024", "Expiration Date": "03/31/2026", "Governing Law": "Poland", "Payment Term": "Net 45", "Contract Name": "Product/ Service Specific Agreement_Polski Dostawca ≈ªywno≈õci Sp. z o.o._04/01/2024", "Scope Category level 1": "Operations and Logistics", "source_language": "pl", "confidence": "high"}

Example 3 (Estonian IT service agreement - fixed term):
Input: "Circle K Eesti AS... TechServices O√ú... IT teenuste leping... j√µustub 1. aprillil 2024... kehtib kuni 31. m√§rtsini 2025... tarkvara ja IT tugi..."
Output: {"Original File Name": "estonian_it_service_agreement.pdf", "Counterparty Legal Entity Name": "TechServices O√ú", "Internal Contracting Entity": "Circle K Eesti AS", "Contract Type": "Product/ Service Specific Agreement", "Term Type": "Fixed", "Effective Date": "04/01/2024", "Expiration Date": "03/31/2025", "Governing Law": "Estonia", "Payment Term": "Net 30", "Contract Name": "Product/ Service Specific Agreement_TechServices O√ú_04/01/2024", "Scope Category level 1": "Technology", "source_language": "et", "confidence": "high"}

RULES:
1. Original File Name must include the file extension (.pdf, .txt, .docx)
2. Use null for fields not found or not applicable
3. Always use MM/DD/YYYY for dates
4. Keep company names in original form
5. Translate legal terms to English
6. Expiration Date must be null for \"Perpetual\" term type
7. Contract Name format: {Contract Type}_{Supplier Name}_{Effective Date in MM/DD/YYYY}
8. Include \"source_language\" field (detected: sv, pl, et, no, da, lv, lt)
9. Include \"confidence\" field: \"high\", \"medium\", or \"low\"
10. Include \"extraction_notes\" for uncertainties

Return ONLY valid JSON."""


def save_text_as_markdown(text, blob_name, container_name=MARKDOWN_CONTAINER):
    """Save extracted text as markdown to blob storage"""
    if not blob_service_client or not SAVE_MARKDOWN:
        return False
    
    try:
        # Change extension to .md
        markdown_name = os.path.splitext(blob_name)[0] + '.md'
        
        # Add markdown header with metadata
        markdown_content = f"# Extracted Text\n\n"
        markdown_content += f"**Source File:** {blob_name}\n\n"
        markdown_content += f"**Extraction Date:** {datetime.now().isoformat()}\n\n"
        markdown_content += f"---\n\n{text}"
        
        # Get blob client for markdown container
        container_client = blob_service_client.get_container_client(container_name)
        
        # Create container if it doesn't exist
        try:
            container_client.create_container()
        except:
            pass  # Container already exists
        
        # Upload markdown file
        blob_client = container_client.get_blob_client(markdown_name)
        blob_client.upload_blob(markdown_content, overwrite=True)
        
        return True
    except Exception as e:
        print(f"Warning: Could not save markdown for {blob_name}: {e}")
        return False


def extract_text_from_docx(file_data):
    """Extract text from Word document (bytes or file path)"""
    try:
        if isinstance(file_data, bytes):
            # Handle bytes data from blob storage
            doc = Document(io.BytesIO(file_data))
        else:
            # Handle file path
            doc = Document(file_data)
        
        full_text = []
        for paragraph in doc.paragraphs:
            full_text.append(paragraph.text)
        return '\n'.join(full_text), None
    except Exception as e:
        return None, str(e)


def extract_text_from_pdf(file_data, blob_url=None):
    """Extract text from PDF using Azure Document Intelligence OCR
    
    Args:
        file_data: Bytes data or file path (can be None if blob_url provided)
        blob_url: Optional blob URL for managed identity access
    """
    if not doc_client:
        return None, "Document Intelligence not configured"
    
    try:
        if blob_url:
            # Use managed identity - Document Intelligence accesses blob directly
            poller = doc_client.begin_analyze_document_from_url(
                "prebuilt-read",
                document_url=blob_url
            )
        elif isinstance(file_data, bytes):
            # Handle bytes data from blob storage
            poller = doc_client.begin_analyze_document(
                "prebuilt-read",
                document=file_data
            )
        elif file_data:
            # Handle file path
            with open(file_data, "rb") as f:
                poller = doc_client.begin_analyze_document(
                    "prebuilt-read",
                    document=f
                )
        else:
            return None, "No file data or blob URL provided"
        
        result = poller.result()
        
        # Extract all text content
        text_content = []
        for page in result.pages:
            for line in page.lines:
                text_content.append(line.content)
        
        return '\n'.join(text_content), None
        
    except Exception as e:
        return None, str(e)


def read_contract_file(file_path_or_data, extension=None, blob_url=None):
    """Read contract file based on extension (.txt, .docx, .pdf)
    
    Args:
        file_path_or_data: Either a file path string or bytes data from blob
        extension: File extension (required if file_path_or_data is bytes)
        blob_url: Optional blob URL for PDF managed identity access
    """
    if isinstance(file_path_or_data, bytes):
        # Reading from blob storage (bytes data)
        if not extension:
            return None, "Extension required for bytes data"
        
        ext = extension.lower()
        
        if ext == '.txt':
            try:
                return file_path_or_data.decode('utf-8'), None
            except Exception as e:
                return None, str(e)
        
        elif ext == '.docx':
            return extract_text_from_docx(file_path_or_data)
        
        elif ext == '.pdf':
            return extract_text_from_pdf(file_path_or_data, blob_url=blob_url)
        
        else:
            return None, f"Unsupported file format: {ext}"
    
    elif file_path_or_data is None and blob_url:
        # Blob URL provided without data (for PDFs using managed identity)
        if not extension:
            return None, "Extension required for blob URL"
        
        ext = extension.lower()
        if ext == '.pdf':
            return extract_text_from_pdf(None, blob_url=blob_url)
        else:
            return None, f"Blob URL only supported for PDFs, got: {ext}"
    
    else:
        # Reading from local file path
        path = Path(file_path_or_data)
        ext = path.suffix.lower()
        
        if ext == '.txt':
            try:
                with open(file_path_or_data, 'r', encoding='utf-8') as f:
                    return f.read(), None
            except Exception as e:
                return None, str(e)
        
        elif ext == '.docx':
            return extract_text_from_docx(file_path_or_data)
        
        elif ext == '.pdf':
            return extract_text_from_pdf(file_path_or_data, blob_url=blob_url)
        
        else:
            return None, f"Unsupported file format: {ext}"


def extract_metadata_direct(contract_text, file_name="", attempt=1):
    """
    Extract metadata directly from original language contract
    No translation needed - GPT-4o-mini reads multiple languages
    
    Args:
        contract_text: Full contract text
        file_name: Filename for context and Original File Name field
        attempt: Current retry attempt (for logging)
    """
    
    user_prompt = f"""CRITICAL INSTRUCTION - ORIGINAL FILE NAME:
The actual filename for this document is: {file_name}
You MUST use "{file_name}" EXACTLY as the "Original File Name" field value.
DO NOT create or infer a filename from document content or titles.
REQUIRED VALUE: {file_name}

Now extract metadata from this contract and return all values in ENGLISH:

CONTRACT TEXT:
{contract_text[:100000]}

Return JSON with the 12 required Sirion fields, plus source_language and confidence.
REMEMBER: "Original File Name" must be exactly: {file_name}
IMPORTANT: For "Related Master Agreement", use the format: "{{Document Type}} to: {{Parent Agreement}}". Example: "Amendment to: Master Supply Agreement_04/01/2024"."""

    try:
        response = client.chat.completions.create(
            model=DEPLOYMENT_NAME,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            max_completion_tokens=2000
        )
        
        result = json.loads(response.choices[0].message.content)
        result['file_name'] = file_name
        result['extraction_timestamp'] = datetime.now().isoformat()
        
        # Override Original File Name to ensure it's always the actual filename
        result['Original File Name'] = file_name
        
        return result, None
        
    except Exception as e:
        error_msg = str(e)
        if "rate" in error_msg.lower() or "429" in error_msg:
            error_msg += " (Rate limit detected)"
        return None, error_msg


def validate_critical_fields(metadata):
    """Validate that critical fields are not empty"""
    missing_fields = []
    
    # Critical fields that should never be empty
    critical_fields = [
        'Internal Contracting Entity',
        'Counterparty Legal Entity Name',
        'Effective Date',
        'Contract Type'
    ]
    
    for field in critical_fields:
        value = metadata.get(field)
        if not value or value == '' or value == 'null' or value is None:
            missing_fields.append(field)
    
    return missing_fields


def analyze_field_quality(metadata):
    """Analyze which fields are missing or empty for quality reporting"""
    issues = []
    
    # Check all Sirion fields
    for field in SIRION_FIELDS:
        value = metadata.get(field)
        if not value or value == '' or value == 'null' or value is None:
            issues.append(f"Missing: {field}")
    
    return issues


def translate_to_english(contract_text, file_name=""):
    """Translate contract to English for improved OCR/PDF extraction"""
    translation_prompt = f"""Translate this contract to English. Preserve:
- All company names exactly as written
- All dates in YYYY-MM-DD format
- Numbers and amounts
- Legal terminology accurately
- Document structure

CONTRACT TEXT:
{contract_text[:100000]}

Return ONLY the English translation, preserving structure and formatting."""
    
    try:
        response = client.chat.completions.create(
            model=DEPLOYMENT_NAME,
            messages=[
                {"role": "system", "content": "You are a professional legal translator. Translate accurately while preserving dates, names, and legal terms."},
                {"role": "user", "content": translation_prompt}
            ],
            max_completion_tokens=16000
        )
        
        translation = response.choices[0].message.content
        return translation, None
        
    except Exception as e:
        return None, str(e)


def extract_metadata_from_english(english_text, file_name=""):
    """Extract metadata from already-translated English contract"""
    
    user_prompt = f"""Extract metadata from this English contract:

CONTRACT TEXT:
{english_text[:16000]}

Return JSON with the 12 required Sirion fields, plus confidence."""

    try:
        response = client.chat.completions.create(
            model=DEPLOYMENT_NAME,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            max_completion_tokens=1500
        )
        
        result = json.loads(response.choices[0].message.content)
        result['file_name'] = file_name
        result['extraction_timestamp'] = datetime.now().isoformat()
        result['extraction_method'] = 'two_call_translation'
        
        # Override Original File Name to ensure it's always the actual filename
        result['Original File Name'] = file_name
        
        return result, None
        
    except Exception as e:
        return None, str(e)


def process_contract_with_retry(file_info, progress_tracker=None):
    """
    Process a single contract with retry logic
    Designed for parallel execution
    
    Args:
        file_info: Dict with 'name', 'data', 'extension', 'blob_url'
        progress_tracker: ProgressTracker instance for thread-safe counting
    
    Returns:
        Dict with extraction results or error info
    """
    file_name = file_info['name']
    file_data = file_info['data']
    extension = file_info['extension']
    blob_url = file_info.get('blob_url')
    
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            # Read contract file
            contract_text, read_error = read_contract_file(file_data, extension, blob_url=blob_url)
            
            if not contract_text:
                folder_path = os.path.dirname(file_name) if '/' in file_name or '\\' in file_name else ''
                base_name = os.path.basename(file_name)
                result = {
                    'Folder Path': folder_path,
                    'File Name': base_name,
                    'Error': f"Read error: {read_error}"
                }
                if progress_tracker:
                    completed = progress_tracker.increment()
                    print(f"   [{completed}/{progress_tracker.total}] {file_name}: ‚ùå Read failed")
                return result
            
            # Save extracted text as markdown to blob storage
            if SAVE_MARKDOWN:
                save_text_as_markdown(contract_text, file_name)
            
            # Determine extraction strategy
            is_pdf = extension.lower() == '.pdf'
            use_two_call = ALWAYS_USE_TWO_CALL or (is_pdf and USE_TWO_CALL_FOR_PDFS)
            
            # Extract metadata
            if use_two_call:
                # Two-call approach: translate then extract (better for OCR'd PDFs)
                translation, trans_error = translate_to_english(contract_text, file_name)
                if not translation:
                    folder_path = os.path.dirname(file_name) if '/' in file_name or '\\' in file_name else ''
                    base_name = os.path.basename(file_name)
                    result = {
                        'Folder Path': folder_path,
                        'File Name': base_name,
                        'Error': f"Translation error: {trans_error}"
                    }
                    if progress_tracker:
                        completed = progress_tracker.increment()
                        print(f"   [{completed}/{progress_tracker.total}] {file_name}: ‚ùå Translation failed")
                    return result
                
                metadata, error = extract_metadata_from_english(translation, file_name)
            else:
                # Single-call approach: direct extraction
                metadata, error = extract_metadata_direct(contract_text, file_name, attempt)
            
            if metadata:
                # Build result row
                # Split path into folder and file name
                folder_path = os.path.dirname(file_name) if '/' in file_name or '\\' in file_name else ''
                base_name = os.path.basename(file_name)
                
                row = {
                    'Folder Path': folder_path,
                    'File Name': base_name,
                    'Source Language': metadata.get('source_language', 'unknown'),
                    'Extraction Timestamp': metadata.get('extraction_timestamp', ''),
                }
                
                # Add all 11 Sirion fields
                for field in SIRION_FIELDS:
                    row[field] = metadata.get(field, '')
                
                # Add quality indicators
                row['Confidence'] = metadata.get('confidence', 'medium')
                row['Notes'] = metadata.get('extraction_notes', '')
                
                if progress_tracker:
                    completed = progress_tracker.increment()
                    lang = metadata.get('source_language', 'unknown').upper()
                    conf = metadata.get('confidence', 'medium')
                    
                    # Basic status line
                    print(f"   [{completed}/{progress_tracker.total}] {file_name}: ‚úÖ {lang}, {conf}")
                    
                    # For low or medium confidence, show contract name and detailed field analysis
                    if conf in ['low', 'medium']:
                        contract_name = metadata.get('Contract Name', 'N/A')
                        print(f"      üìã Contract: {contract_name}")
                        
                        field_issues = analyze_field_quality(metadata)
                        if field_issues:
                            print(f"      ‚ö†Ô∏è  Quality Issues: {', '.join(field_issues)}")
                
                return row
            
            # Handle errors with retry logic
            elif attempt < MAX_RETRIES and ("rate" in error.lower() or "429" in error):
                # Rate limited - retry with exponential backoff
                wait_time = RETRY_DELAY * (2 ** (attempt - 1))
                if progress_tracker:
                    print(f"   [{progress_tracker.completed}/{progress_tracker.total}] {file_name}: ‚è∏Ô∏è Rate limited, retrying in {wait_time}s (attempt {attempt}/{MAX_RETRIES})")
                time.sleep(wait_time)
                continue
            else:
                # Non-retryable error or max retries reached
                folder_path = os.path.dirname(file_name) if '/' in file_name or '\\' in file_name else ''
                base_name = os.path.basename(file_name)
                result = {
                    'Folder Path': folder_path,
                    'File Name': base_name,
                    'Error': error
                }
                if progress_tracker:
                    completed = progress_tracker.increment()
                    print(f"   [{completed}/{progress_tracker.total}] {file_name}: ‚ùå {error}")
                return result
                
        except Exception as e:
            if attempt < MAX_RETRIES:
                wait_time = RETRY_DELAY * (2 ** (attempt - 1))
                if progress_tracker:
                    print(f"   [{progress_tracker.completed}/{progress_tracker.total}] {file_name}: ‚è∏Ô∏è Error, retrying in {wait_time}s")
                time.sleep(wait_time)
                continue
            else:
                folder_path = os.path.dirname(file_name) if '/' in file_name or '\\' in file_name else ''
                base_name = os.path.basename(file_name)
                result = {
                    'Folder Path': folder_path,
                    'File Name': base_name,
                    'Error': str(e)
                }
                if progress_tracker:
                    completed = progress_tracker.increment()
                    print(f"   [{completed}/{progress_tracker.total}] {file_name}: ‚ùå {str(e)}")
                return result
    
    # Should not reach here, but handle edge case
    folder_path = os.path.dirname(file_name) if '/' in file_name or '\\' in file_name else ''
    base_name = os.path.basename(file_name)
    return {
        'Folder Path': folder_path,
        'File Name': base_name,
        'Error': 'Max retries exceeded'
    }


def process_contracts_from_blob(container_name, output_csv="sirion_metadata.csv", parallel=True):
    """
    Process all contracts from Azure Blob Storage and export to CSV
    Supports .txt, .docx, and .pdf files
    
    Args:
        container_name: Azure Blob Storage container name
        output_csv: Output CSV file name
    """
    
    if not blob_service_client:
        print("‚ùå Azure Blob Storage not configured")
        print("   Set STORAGE_ACCOUNT_NAME in .env file")
        return None
    
    try:
        container_client = blob_service_client.get_container_client(container_name)
        
        # List all blobs with supported extensions
        print(f"\n{'='*80}")
        print(f"üåç Direct Metadata Extraction from Azure Blob Storage")
        print(f"{'='*80}")
        print(f"üì¶ Container: {container_name}")
        print(f"üîç Scanning for contracts...")
        
        blobs = []
        for blob in container_client.list_blobs():
            ext = Path(blob.name).suffix.lower()
            if ext in ['.txt', '.docx', '.pdf']:
                blobs.append(blob)
        
        if not blobs:
            print(f"‚ö†Ô∏è No contract files found in container '{container_name}'")
            print(f"   Looking for: .txt, .docx, .pdf files")
            return None
        
        print(f"üìä Files Found: {len(blobs)}")
        print(f"üìÑ Formats: .txt, .docx, .pdf (with OCR)")
        print(f"ü§ñ Model: {DEPLOYMENT_NAME}")
        if parallel:
            print(f"‚ö° Parallel Mode: {MAX_WORKERS} concurrent workers")
        else:
            print(f"üê¢ Sequential Mode: One at a time")
        print(f"üíæ Output: {output_csv}")
        print(f"{'='*80}\n")
        
        # Prepare file info for processing
        file_infos = []
        for blob in blobs:
            try:
                blob_client = container_client.get_blob_client(blob.name)
                extension = Path(blob.name).suffix.lower()
                
                # For PDFs, use blob URL with managed identity
                # For .txt/.docx, download bytes
                if extension == '.pdf':
                    blob_url = blob_client.url
                    blob_data = None
                else:
                    blob_url = None
                    blob_data = blob_client.download_blob().readall()
                
                file_infos.append({
                    'name': blob.name,
                    'data': blob_data,
                    'extension': extension,
                    'blob_url': blob_url
                })
            except Exception as e:
                print(f"‚ùå Failed to prepare {blob.name}: {str(e)}")
                file_infos.append({
                    'name': blob.name,
                    'data': None,
                    'extension': None,
                    'blob_url': None
                })
        
        # Process contracts (parallel or sequential)
        results = []
        progress = ProgressTracker(len(file_infos))
        
        if parallel and len(file_infos) > 1:
            # Parallel processing
            print(f"üöÄ Processing {len(file_infos)} contracts in parallel...\n")
            
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = {
                    executor.submit(process_contract_with_retry, file_info, progress): file_info['name']
                    for file_info in file_infos
                }
                
                for future in as_completed(futures):
                    results.append(future.result())
        else:
            # Sequential processing
            print(f"Processing {len(file_infos)} contracts sequentially...\n")
            for file_info in file_infos:
                results.append(process_contract_with_retry(file_info, progress))
        
        # Count successes/failures
        success_count = sum(1 for r in results if 'Error' not in r)
        error_count = sum(1 for r in results if 'Error' in r)
        
        # Export to CSV (same as local version)
        if results:
            csv_columns = ['Folder Path', 'File Name', 'Source Language', 'Extraction Timestamp'] + SIRION_FIELDS + ['Confidence', 'Notes']
            
            with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=csv_columns, extrasaction='ignore')
                writer.writeheader()
                writer.writerows(results)
            
            # Export failed contracts list if any failures
            if error_count > 0:
                failed_file = output_csv.replace('.csv', '_failed_contracts.txt')
                failed_contracts = [r.get('File Name', r.get('Folder Path', 'Unknown')) for r in results if 'Error' in r]
                with open(failed_file, 'w', encoding='utf-8') as f:
                    f.write(f"Failed Contracts ({error_count} total)\n")
                    f.write("="*80 + "\n\n")
                    for contract in failed_contracts:
                        f.write(f"{contract}\n")
                print(f"üíæ Failed contracts list: {failed_file}")
            
            print(f"\n{'='*80}")
            print(f"‚úÖ Extraction Complete!")
            print(f"{'='*80}")
            print(f"üìä Total Files: {len(blobs)}")
            print(f"‚úÖ Successful: {success_count}")
            print(f"‚ùå Failed: {error_count}")
            print(f"üíæ Output: {output_csv}")
            
            # Summary statistics (same as local version)
            if success_count > 0:
                languages = {}
                confidences = {'high': 0, 'medium': 0, 'low': 0}
                
                for row in results:
                    if 'Source Language' in row:
                        lang = row.get('Source Language', 'unknown')
                        languages[lang] = languages.get(lang, 0) + 1
                    
                    if 'Confidence' in row:
                        conf = row.get('Confidence', 'medium')
                        if conf in confidences:
                            confidences[conf] += 1
                
                print(f"\nüåç Languages Detected:")
                for lang, count in languages.items():
                    print(f"   {lang.upper()}: {count} contracts")
                
                print(f"\nüìà Confidence Distribution:")
                for conf, count in confidences.items():
                    print(f"   {conf.title()}: {count} contracts")
                
                # Calculate cost
                avg_tokens_per_doc = 2000
                total_tokens = success_count * avg_tokens_per_doc
                cost_per_million = 0.15
                estimated_cost = (total_tokens / 1_000_000) * cost_per_million
                
                print(f"\nüí∞ Cost Estimate:")
                print(f"   Total Tokens: ~{total_tokens:,}")
                print(f"   Estimated Cost: ${estimated_cost:.2f}")
            
            print(f"\nüí° Next Steps:")
            print(f"   1. Open {output_csv} in Excel")
            print(f"   2. Review contracts with 'low' or 'medium' confidence")
            print(f"   3. Verify company names and dates")
            print(f"   4. Import to Sirion.ai CLM platform")
            print(f"\n‚ú® Benefits of Direct Extraction:")
            print(f"   ‚úÖ No translation needed (98% cost savings)")
            print(f"   ‚úÖ Single-step process (60% faster)")
            print(f"   ‚úÖ Original legal terminology preserved")
            print(f"   ‚úÖ Supports .txt, .docx, .pdf formats")
            print(f"   ‚úÖ Direct access from Azure Blob Storage")
            print(f"\n{'='*80}\n")
        
        return output_csv
        
    except Exception as e:
        print(f"‚ùå Error accessing blob storage: {str(e)}")
        print(f"   Container: {container_name}")
        print(f"   Storage Account: {storage_account_name}")
        return None


def process_contracts_to_csv(input_folder, output_csv="sirion_metadata.csv", parallel=True):
    """
    Process all contracts and export to CSV
    Supports .txt, .docx, and .pdf files
    
    Args:
        input_folder: Folder containing original contracts
        output_csv: Output CSV file name
        parallel: Use parallel processing (default: True)
    """
    
    input_path = Path(input_folder)
    if not input_path.exists():
        print(f"‚ùå Folder not found: {input_folder}")
        return None
    
    # Find all contract files
    files = []
    for ext in ['*.txt', '*.docx', '*.pdf']:
        files.extend(list(input_path.glob(ext)))
    
    if not files:
        print(f"‚ö†Ô∏è No contract files found in {input_folder}")
        print(f"   Looking for: .txt, .docx, .pdf files")
        return None
    
    print(f"\n{'='*80}")
    print(f"üåç Direct Metadata Extraction - No Translation Needed!")
    print(f"{'='*80}")
    print(f"üìÅ Input Folder: {input_folder}")
    print(f"üìä Files Found: {len(files)}")
    print(f"üìÑ Formats: .txt, .docx, .pdf (with OCR)")
    print(f"ü§ñ Model: {DEPLOYMENT_NAME}")
    if parallel:
        print(f"‚ö° Parallel Mode: {MAX_WORKERS} concurrent workers")
    else:
        print(f"üê¢ Sequential Mode: One at a time")
    print(f"üíæ Output: {output_csv}")
    print(f"{'='*80}\n")
    
    # Prepare file info for processing
    file_infos = []
    for file_path in files:
        file_infos.append({
            'name': file_path.name,
            'data': str(file_path),  # Pass path as string
            'extension': file_path.suffix.lower()
        })
    
    # Process contracts
    results = []
    progress = ProgressTracker(len(file_infos))
    
    if parallel and len(file_infos) > 1:
        # Parallel processing
        print(f"üöÄ Processing {len(file_infos)} contracts in parallel...\n")
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = {
                executor.submit(process_contract_with_retry, file_info, progress): file_info['name']
                for file_info in file_infos
            }
            
            for future in as_completed(futures):
                results.append(future.result())
    else:
        # Sequential processing
        print(f"Processing {len(file_infos)} contracts sequentially...\n")
        for file_info in file_infos:
            results.append(process_contract_with_retry(file_info, progress))
    
    # Count successes/failures
    success_count = sum(1 for r in results if 'Error' not in r)
    error_count = sum(1 for r in results if 'Error' in r)
    
    # Export to CSV
    if results:
        # Define CSV columns
        csv_columns = ['File Name', 'Source Language', 'Extraction Timestamp'] + SIRION_FIELDS + ['Confidence', 'Notes']
        
        # Write CSV
        with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=csv_columns, extrasaction='ignore')
            writer.writeheader()
            writer.writerows(results)
        
        # Export failed contracts list if any failures
        if error_count > 0:
            failed_file = output_csv.replace('.csv', '_failed_contracts.txt')
            failed_contracts = [r.get('File Name', 'Unknown') for r in results if 'Error' in r]
            with open(failed_file, 'w', encoding='utf-8') as f:
                f.write(f"Failed Contracts ({error_count} total)\n")
                f.write("="*80 + "\n\n")
                for contract in failed_contracts:
                    f.write(f"{contract}\n")
            print(f"üíæ Failed contracts list: {failed_file}")
        
        print(f"\n{'='*80}")
        print(f"‚úÖ Extraction Complete!")
        print(f"{'='*80}")
        print(f"üìä Total Files: {len(files)}")
        print(f"‚úÖ Successful: {success_count}")
        print(f"‚ùå Failed: {error_count}")
        print(f"üíæ Output: {output_csv}")
        
        # Summary statistics
        if success_count > 0:
            languages = {}
            confidences = {'high': 0, 'medium': 0, 'low': 0}
            
            for row in results:
                if 'Source Language' in row:
                    lang = row.get('Source Language', 'unknown')
                    languages[lang] = languages.get(lang, 0) + 1
                
                if 'Confidence' in row:
                    conf = row.get('Confidence', 'medium')
                    if conf in confidences:
                        confidences[conf] += 1
            
            print(f"\nüåç Languages Detected:")
            for lang, count in languages.items():
                print(f"   {lang.upper()}: {count} contracts")
            
            print(f"\nüìà Confidence Distribution:")
            for conf, count in confidences.items():
                print(f"   {conf.title()}: {count} contracts")
            
            # Calculate cost
            avg_tokens_per_doc = 2000
            total_tokens = success_count * avg_tokens_per_doc
            cost_per_million = 0.15  # GPT-4o-mini input cost
            estimated_cost = (total_tokens / 1_000_000) * cost_per_million
            
            print(f"\nüí∞ Cost Estimate:")
            print(f"   Total Tokens: ~{total_tokens:,}")
            print(f"   Estimated Cost: ${estimated_cost:.2f}")
        
        print(f"\nüí° Next Steps:")
        print(f"   1. Open {output_csv} in Excel")
        print(f"   2. Review contracts with 'low' or 'medium' confidence")
        print(f"   3. Verify company names and dates")
        print(f"   4. Import to Sirion.ai CLM platform")
        print(f"\n‚ú® Benefits of Direct Extraction:")
        print(f"   ‚úÖ No translation needed (98% cost savings)")
        print(f"   ‚úÖ Single-step process (60% faster)")
        print(f"   ‚úÖ Original legal terminology preserved")
        print(f"   ‚úÖ Supports .txt, .docx, .pdf formats")
        print(f"\n{'='*80}\n")
    
    return output_csv


def process_single_contract(file_path):
    """Test extraction on a single contract"""
    
    path = Path(file_path)
    if not path.exists():
        print(f"‚ùå File not found: {file_path}")
        return None
    
    print(f"\nüîç Testing Direct Metadata Extraction")
    print(f"üìÑ File: {path.name}\n")
    
    # Read contract file (handles .txt, .docx, .pdf)
    contract_text, read_error = read_contract_file(path)
    
    if not contract_text:
        print(f"‚ùå Failed to read file: {read_error}")
        return None
    
    print(f"üìù Contract Length: {len(contract_text)} characters")
    print(f"ü§ñ Processing with {DEPLOYMENT_NAME}...\n")
    
    metadata, error = extract_metadata_direct(contract_text, path.name)
    
    if metadata:
        print("‚úÖ Extraction Successful!\n")
        print(json.dumps(metadata, indent=2))
        
        # Save to JSON
        output_json = path.stem + "_metadata.json"
        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2)
        print(f"\nüíæ Saved to: {output_json}")
        
        return metadata
    else:
        print(f"‚ùå Extraction Failed: {error}")
        return None


if __name__ == "__main__":
    import sys
    
    print("\n" + "="*80)
    print("üåç Circle K Contract Metadata Extractor - Direct Extraction Mode")
    print("="*80)
    print("üìã Extracts metadata from original language contracts (no translation)")
    print("ü§ñ Uses GPT-4o-mini multilingual capabilities")
    print("üìÑ Supports: .txt, .docx, .pdf (with OCR)")
    print("üíæ Outputs to CSV for Sirion.ai import")
    print("üì¶ Source: Azure Blob Storage or Local Files")
    print("="*80 + "\n")
    
    if len(sys.argv) > 1:
        # Single file mode
        file_path = sys.argv[1]
        process_single_contract(file_path)
    else:
        # Ask user for source type
        print("Select source:")
        print("  1. Azure Blob Storage (recommended)")
        print("  2. Local folder")
        source_choice = input("\nEnter choice (1 or 2, default=1): ").strip()
        
        if source_choice == "2":
            # Local folder mode
            input_folder = input("Enter folder path with contracts (or press Enter for 'sample_contracts'): ").strip()
            if not input_folder:
                input_folder = "sample_contracts"
            
            output_csv = input("Enter output CSV name (or press Enter for 'sirion_metadata.csv'): ").strip()
            if not output_csv:
                output_csv = "sirion_metadata.csv"
            
            process_contracts_to_csv(input_folder, output_csv)
        else:
            # Azure Blob Storage mode (default)
            container_name = input("Enter Azure Blob Storage container name (or press Enter for 'documents'): ").strip()
            if not container_name:
                container_name = "documents"
            
            output_csv = input("Enter output CSV name (or press Enter for 'sirion_metadata.csv'): ").strip()
            if not output_csv:
                output_csv = "sirion_metadata.csv"
            
            process_contracts_from_blob(container_name, output_csv)
