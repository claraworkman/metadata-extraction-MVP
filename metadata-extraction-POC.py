"""
Direct Metadata Extraction from Original Contracts
Extracts metadata from Swedish, Polish, Estonian, etc. contracts without translation
Supports .txt, .docx, and .pdf files
Outputs to CSV for easy download and Sirion import
"""

import os
import json
import csv
from pathlib import Path
from dotenv import load_dotenv
from openai import AzureOpenAI
from azure.identity import DefaultAzureCredential, get_bearer_token_provider
from datetime import datetime
import time
from docx import Document
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.storage.blob import BlobServiceClient
import io
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

load_dotenv()

# Azure OpenAI Configuration with Azure AD authentication
credential = DefaultAzureCredential()
token_provider = get_bearer_token_provider(
    credential, 
    "https://cognitiveservices.azure.com/.default"
)

client = AzureOpenAI(
    azure_ad_token_provider=token_provider,
    api_version="2025-01-01-preview",
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
)

# Azure Document Intelligence client for PDF OCR
document_intelligence_endpoint = os.getenv("DOCUMENT_INTELLIGENCE_ENDPOINT")
if document_intelligence_endpoint:
    doc_client = DocumentAnalysisClient(
        endpoint=document_intelligence_endpoint,
        credential=credential
    )
else:
    doc_client = None

# Azure Blob Storage client
storage_account_name = os.getenv("STORAGE_ACCOUNT_NAME")
if storage_account_name:
    blob_service_client = BlobServiceClient(
        account_url=f"https://{storage_account_name}.blob.core.windows.net",
        credential=credential
    )
else:
    blob_service_client = None

DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")

# Accuracy configuration
USE_TWO_CALL_FOR_PDFS = os.getenv("USE_TWO_CALL_FOR_PDFS", "true").lower() == "true"  # Always translate PDFs first
ALWAYS_USE_TWO_CALL = os.getenv("ALWAYS_USE_TWO_CALL", "false").lower() == "true"  # Force translation for all files

# Parallel processing configuration
MAX_WORKERS = int(os.getenv("MAX_WORKERS", "10"))  # Concurrent threads
MAX_RETRIES = int(os.getenv("MAX_RETRIES", "3"))   # Retry attempts per file
RETRY_DELAY = float(os.getenv("RETRY_DELAY", "2")) # Seconds between retries

# Thread-safe progress tracking
class ProgressTracker:
    def __init__(self, total):
        self.total = total
        self.completed = 0
        self.lock = threading.Lock()
    
    def increment(self):
        with self.lock:
            self.completed += 1
            return self.completed

# Sirion metadata fields
SIRION_FIELDS = [
    "Customer (CK) Entity",
    "Supplier Entity",
    "Effective Date",
    "Expiration Date",
    "Term Type",
    "Governing Law",
    "Contract Type",
    "Contract Currency",
    "Payment Term",
    "Termination for Convenience",
    "Notice Period for Termination for Convenience",
    "Party with the Right to Terminate for Convenience"
]

SYSTEM_PROMPT = """You are a multilingual contract metadata extraction specialist. You can read contracts in Swedish, Norwegian, Danish, Polish, Latvian, Lithuanian, and Estonian.

Extract the following 12 fields from contract documents. READ the contract in its original language, but RETURN all field values in ENGLISH:

Required fields (return values in ENGLISH):
1. Customer (CK) Entity - The Circle K entity name (keep original)
2. Supplier Entity - The supplier company name (keep original)
3. Effective Date - Contract start date (YYYY-MM-DD format)
4. Expiration Date - Contract end date (YYYY-MM-DD) or null if indefinite
5. Term Type - "Fixed term", "Evergreen", "Auto-renewing", etc. (English)
6. Governing Law - Jurisdiction (English, e.g., "Swedish law", "Polish law")
7. Contract Type - e.g., "Supply Agreement" (translate to English)
8. Contract Currency - Currency code (USD, EUR, SEK, PLN, etc.)
9. Payment Term - e.g., "Net 30", "Net 60" (English)
10. Termination for Convenience - "Yes" or "No"
11. Notice Period for Termination for Convenience - e.g., "30 days" or null
12. Party with the Right to Terminate for Convenience - "Both parties", "Customer only", "Supplier only", or null

TRANSLATION EXAMPLES:
- Swedish "Leveransavtal" ‚Üí "Supply Agreement"
- Polish "Umowa Dostawy" ‚Üí "Supply Agreement"
- Estonian "Tarnekokkulepe" ‚Üí "Supply Agreement"
- Swedish "svensk lag" ‚Üí "Swedish law"
- Polish "prawo polskie" ‚Üí "Polish law"
- Estonian "Eesti √µigus" ‚Üí "Estonian law"

CRITICAL: NEVER leave Customer (CK) Entity or Supplier Entity empty. Search the entire contract for company names.

FEW-SHOT EXAMPLES:

Example 1 (Swedish evergreen contract):
Input: "Circle K Sverige AB... Scandinavian Food Suppliers AB... tr√§der i kraft den 1 april 2024... l√∂per p√• obest√§md tid... upps√§gning med 90 dagars varsel..."
Output: {"Customer (CK) Entity": "Circle K Sverige AB", "Supplier Entity": "Scandinavian Food Suppliers AB", "Effective Date": "2024-04-01", "Expiration Date": null, "Term Type": "Evergreen", "Governing Law": "Swedish law", "Contract Type": "Supply Agreement", "Contract Currency": "SEK", "Payment Term": "Net 60", "Termination for Convenience": "Yes", "Notice Period for Termination for Convenience": "90 days", "Party with the Right to Terminate for Convenience": "Both parties", "source_language": "sv", "confidence": "high"}

Example 2 (Polish auto-renewing contract):
Input: "Circle K Polska Sp. z o.o.... Polski Dostawca ≈ªywno≈õci Sp. z o.o.... wchodzi w ≈ºycie 1 kwietnia 2024... obowiƒÖzuje przez okres 24 miesiƒôcy... automatycznie przed≈Çu≈ºa siƒô... wypowiedzenie z 60-dniowym wyprzedzeniem..."
Output: {"Customer (CK) Entity": "Circle K Polska Sp. z o.o.", "Supplier Entity": "Polski Dostawca ≈ªywno≈õci Sp. z o.o.", "Effective Date": "2024-04-01", "Expiration Date": "2026-04-01", "Term Type": "Fixed term with auto-renewal", "Governing Law": "Polish law", "Contract Type": "Supply Agreement", "Contract Currency": "PLN", "Payment Term": "Net 45", "Termination for Convenience": "Yes", "Notice Period for Termination for Convenience": "60 days", "Party with the Right to Terminate for Convenience": "Both parties", "source_language": "pl", "confidence": "high"}

Example 3 (Estonian indefinite contract):
Input: "Circle K Eesti AS... Eesti Toidutarnijad O√ú... j√µustub 1. aprillil 2024... kehtib t√§htajatult... v√µib l√µpetada 60 p√§eva etteteatamisega..."
Output: {"Customer (CK) Entity": "Circle K Eesti AS", "Supplier Entity": "Eesti Toidutarnijad O√ú", "Effective Date": "2024-04-01", "Expiration Date": null, "Term Type": "Indefinite", "Governing Law": "Estonian law", "Contract Type": "Supply Agreement", "Contract Currency": "EUR", "Payment Term": "Net 30", "Termination for Convenience": "Yes", "Notice Period for Termination for Convenience": "60 days", "Party with the Right to Terminate for Convenience": "Both parties", "source_language": "et", "confidence": "high"}

RULES:
1. Use null for fields not found or not applicable
2. Always use YYYY-MM-DD for dates
3. Keep company names in original form
4. Translate legal terms to English
5. Include "source_language" field (detected: sv, pl, et, no, da, lv, lt)
6. Include "confidence" field: "high", "medium", or "low"
7. Include "extraction_notes" for uncertainties

Return ONLY valid JSON."""


def extract_text_from_docx(file_data):
    """Extract text from Word document (bytes or file path)"""
    try:
        if isinstance(file_data, bytes):
            # Handle bytes data from blob storage
            doc = Document(io.BytesIO(file_data))
        else:
            # Handle file path
            doc = Document(file_data)
        
        full_text = []
        for paragraph in doc.paragraphs:
            full_text.append(paragraph.text)
        return '\n'.join(full_text), None
    except Exception as e:
        return None, str(e)


def extract_text_from_pdf(file_data):
    """Extract text from PDF using Azure Document Intelligence OCR"""
    if not doc_client:
        return None, "Document Intelligence not configured"
    
    try:
        if isinstance(file_data, bytes):
            # Handle bytes data from blob storage
            poller = doc_client.begin_analyze_document(
                "prebuilt-read",
                document=file_data
            )
        else:
            # Handle file path
            with open(file_data, "rb") as f:
                poller = doc_client.begin_analyze_document(
                    "prebuilt-read",
                    document=f
                )
        
        result = poller.result()
        
        # Extract all text content
        text_content = []
        for page in result.pages:
            for line in page.lines:
                text_content.append(line.content)
        
        return '\n'.join(text_content), None
        
    except Exception as e:
        return None, str(e)


def read_contract_file(file_path_or_data, extension=None):
    """Read contract file based on extension (.txt, .docx, .pdf)
    
    Args:
        file_path_or_data: Either a file path string or bytes data from blob
        extension: File extension (required if file_path_or_data is bytes)
    """
    if isinstance(file_path_or_data, bytes):
        # Reading from blob storage (bytes data)
        if not extension:
            return None, "Extension required for bytes data"
        
        ext = extension.lower()
        
        if ext == '.txt':
            try:
                return file_path_or_data.decode('utf-8'), None
            except Exception as e:
                return None, str(e)
        
        elif ext == '.docx':
            return extract_text_from_docx(file_path_or_data)
        
        elif ext == '.pdf':
            return extract_text_from_pdf(file_path_or_data)
        
        else:
            return None, f"Unsupported file format: {ext}"
    
    else:
        # Reading from local file path
        path = Path(file_path_or_data)
        ext = path.suffix.lower()
        
        if ext == '.txt':
            try:
                with open(file_path_or_data, 'r', encoding='utf-8') as f:
                    return f.read(), None
            except Exception as e:
                return None, str(e)
        
        elif ext == '.docx':
            return extract_text_from_docx(file_path_or_data)
        
        elif ext == '.pdf':
            return extract_text_from_pdf(file_path_or_data)
        
        else:
            return None, f"Unsupported file format: {ext}"


def extract_metadata_direct(contract_text, file_name="", attempt=1):
    """
    Extract metadata directly from original language contract
    No translation needed - GPT-4o-mini reads multiple languages
    
    Args:
        contract_text: Full contract text
        file_name: Filename for context
        attempt: Current retry attempt (for logging)
    """
    
    user_prompt = f"""Extract metadata from this contract and return all values in ENGLISH:

CONTRACT TEXT:
{contract_text[:8000]}

Return JSON with the 12 required Sirion fields, plus source_language and confidence."""

    try:
        response = client.chat.completions.create(
            model=DEPLOYMENT_NAME,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            max_completion_tokens=1000
        )
        
        result = json.loads(response.choices[0].message.content)
        result['file_name'] = file_name
        result['extraction_timestamp'] = datetime.now().isoformat()
        
        return result, None
        
    except Exception as e:
        error_msg = str(e)
        if "rate" in error_msg.lower() or "429" in error_msg:
            error_msg += " (Rate limit detected)"
        return None, error_msg


def validate_critical_fields(metadata):
    """Validate that critical fields are not empty"""
    missing_fields = []
    
    # Critical fields that should never be empty
    critical_fields = [
        'Customer (CK) Entity',
        'Supplier Entity',
        'Effective Date',
        'Contract Type'
    ]
    
    for field in critical_fields:
        value = metadata.get(field)
        if not value or value == '' or value == 'null' or value is None:
            missing_fields.append(field)
    
    return missing_fields


def translate_to_english(contract_text, file_name=""):
    """Translate contract to English for improved OCR/PDF extraction"""
    translation_prompt = f"""Translate this contract to English. Preserve:
- All company names exactly as written
- All dates in YYYY-MM-DD format
- Numbers and amounts
- Legal terminology accurately
- Document structure

CONTRACT TEXT:
{contract_text[:100000]}

Return ONLY the English translation, preserving structure and formatting."""
    
    try:
        response = client.chat.completions.create(
            model=DEPLOYMENT_NAME,
            messages=[
                {"role": "system", "content": "You are a professional legal translator. Translate accurately while preserving dates, names, and legal terms."},
                {"role": "user", "content": translation_prompt}
            ],
            max_completion_tokens=16000
        )
        
        translation = response.choices[0].message.content
        return translation, None
        
    except Exception as e:
        return None, str(e)


def extract_metadata_from_english(english_text, file_name=""):
    """Extract metadata from already-translated English contract"""
    
    user_prompt = f"""Extract metadata from this English contract:

CONTRACT TEXT:
{english_text[:16000]}

Return JSON with the 12 required Sirion fields, plus confidence."""

    try:
        response = client.chat.completions.create(
            model=DEPLOYMENT_NAME,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            max_completion_tokens=1500
        )
        
        result = json.loads(response.choices[0].message.content)
        result['file_name'] = file_name
        result['extraction_timestamp'] = datetime.now().isoformat()
        result['extraction_method'] = 'two_call_translation'
        
        return result, None
        
    except Exception as e:
        return None, str(e)


def process_contract_with_retry(file_info, progress_tracker=None):
    """
    Process a single contract with retry logic
    Designed for parallel execution
    
    Args:
        file_info: Dict with 'name', 'data', 'extension'
        progress_tracker: ProgressTracker instance for thread-safe counting
    
    Returns:
        Dict with extraction results or error info
    """
    file_name = file_info['name']
    file_data = file_info['data']
    extension = file_info['extension']
    
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            # Read contract file
            contract_text, read_error = read_contract_file(file_data, extension)
            
            if not contract_text:
                result = {
                    'File Name': file_name,
                    'Error': f"Read error: {read_error}"
                }
                if progress_tracker:
                    completed = progress_tracker.increment()
                    print(f"   [{completed}/{progress_tracker.total}] {file_name}: ‚ùå Read failed")
                return result
            
            # Determine extraction strategy
            is_pdf = extension.lower() == '.pdf'
            use_two_call = ALWAYS_USE_TWO_CALL or (is_pdf and USE_TWO_CALL_FOR_PDFS)
            
            # Extract metadata
            if use_two_call:
                # Two-call approach: translate then extract (better for OCR'd PDFs)
                translation, trans_error = translate_to_english(contract_text, file_name)
                if not translation:
                    result = {
                        'File Name': file_name,
                        'Error': f"Translation error: {trans_error}"
                    }
                    if progress_tracker:
                        completed = progress_tracker.increment()
                        print(f"   [{completed}/{progress_tracker.total}] {file_name}: ‚ùå Translation failed")
                    return result
                
                metadata, error = extract_metadata_from_english(translation, file_name)
            else:
                # Single-call approach: direct extraction
                metadata, error = extract_metadata_direct(contract_text, file_name, attempt)
            
            if metadata:
                # Build result row
                row = {
                    'File Name': file_name,
                    'Source Language': metadata.get('source_language', 'unknown'),
                    'Extraction Timestamp': metadata.get('extraction_timestamp', ''),
                }
                
                # Add all 12 Sirion fields
                for field in SIRION_FIELDS:
                    row[field] = metadata.get(field, '')
                
                # Add quality indicators
                row['Confidence'] = metadata.get('confidence', 'medium')
                row['Notes'] = metadata.get('extraction_notes', '')
                
                if progress_tracker:
                    completed = progress_tracker.increment()
                    lang = metadata.get('source_language', 'unknown').upper()
                    conf = metadata.get('confidence', 'medium')
                    print(f"   [{completed}/{progress_tracker.total}] {file_name}: ‚úÖ {lang}, {conf}")
                
                return row
            
            # Handle errors with retry logic
            elif attempt < MAX_RETRIES and ("rate" in error.lower() or "429" in error):
                # Rate limited - retry with exponential backoff
                wait_time = RETRY_DELAY * (2 ** (attempt - 1))
                if progress_tracker:
                    print(f"   [{progress_tracker.completed}/{progress_tracker.total}] {file_name}: ‚è∏Ô∏è Rate limited, retrying in {wait_time}s (attempt {attempt}/{MAX_RETRIES})")
                time.sleep(wait_time)
                continue
            else:
                # Non-retryable error or max retries reached
                result = {
                    'File Name': file_name,
                    'Error': error
                }
                if progress_tracker:
                    completed = progress_tracker.increment()
                    print(f"   [{completed}/{progress_tracker.total}] {file_name}: ‚ùå {error}")
                return result
                
        except Exception as e:
            if attempt < MAX_RETRIES:
                wait_time = RETRY_DELAY * (2 ** (attempt - 1))
                if progress_tracker:
                    print(f"   [{progress_tracker.completed}/{progress_tracker.total}] {file_name}: ‚è∏Ô∏è Error, retrying in {wait_time}s")
                time.sleep(wait_time)
                continue
            else:
                result = {
                    'File Name': file_name,
                    'Error': str(e)
                }
                if progress_tracker:
                    completed = progress_tracker.increment()
                    print(f"   [{completed}/{progress_tracker.total}] {file_name}: ‚ùå {str(e)}")
                return result
    
    # Should not reach here, but handle edge case
    return {
        'File Name': file_name,
        'Error': 'Max retries exceeded'
    }


def process_contracts_from_blob(container_name, output_csv="sirion_metadata.csv", parallel=True):
    """
    Process all contracts from Azure Blob Storage and export to CSV
    Supports .txt, .docx, and .pdf files
    
    Args:
        container_name: Azure Blob Storage container name
        output_csv: Output CSV file name
    """
    
    if not blob_service_client:
        print("‚ùå Azure Blob Storage not configured")
        print("   Set STORAGE_ACCOUNT_NAME in .env file")
        return None
    
    try:
        container_client = blob_service_client.get_container_client(container_name)
        
        # List all blobs with supported extensions
        print(f"\n{'='*80}")
        print(f"üåç Direct Metadata Extraction from Azure Blob Storage")
        print(f"{'='*80}")
        print(f"üì¶ Container: {container_name}")
        print(f"üîç Scanning for contracts...")
        
        blobs = []
        for blob in container_client.list_blobs():
            ext = Path(blob.name).suffix.lower()
            if ext in ['.txt', '.docx', '.pdf']:
                blobs.append(blob)
        
        if not blobs:
            print(f"‚ö†Ô∏è No contract files found in container '{container_name}'")
            print(f"   Looking for: .txt, .docx, .pdf files")
            return None
        
        print(f"üìä Files Found: {len(blobs)}")
        print(f"üìÑ Formats: .txt, .docx, .pdf (with OCR)")
        print(f"ü§ñ Model: {DEPLOYMENT_NAME}")
        if parallel:
            print(f"‚ö° Parallel Mode: {MAX_WORKERS} concurrent workers")
        else:
            print(f"üê¢ Sequential Mode: One at a time")
        print(f"üíæ Output: {output_csv}")
        print(f"{'='*80}\n")
        
        # Prepare file info for processing
        file_infos = []
        for blob in blobs:
            try:
                blob_client = container_client.get_blob_client(blob.name)
                blob_data = blob_client.download_blob().readall()
                extension = Path(blob.name).suffix.lower()
                
                file_infos.append({
                    'name': blob.name,
                    'data': blob_data,
                    'extension': extension
                })
            except Exception as e:
                print(f"‚ùå Failed to download {blob.name}: {str(e)}")
                file_infos.append({
                    'name': blob.name,
                    'data': None,
                    'extension': None
                })
        
        # Process contracts (parallel or sequential)
        results = []
        progress = ProgressTracker(len(file_infos))
        
        if parallel and len(file_infos) > 1:
            # Parallel processing
            print(f"üöÄ Processing {len(file_infos)} contracts in parallel...\n")
            
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = {
                    executor.submit(process_contract_with_retry, file_info, progress): file_info['name']
                    for file_info in file_infos
                }
                
                for future in as_completed(futures):
                    results.append(future.result())
        else:
            # Sequential processing
            print(f"Processing {len(file_infos)} contracts sequentially...\n")
            for file_info in file_infos:
                results.append(process_contract_with_retry(file_info, progress))
        
        # Count successes/failures
        success_count = sum(1 for r in results if 'Error' not in r)
        error_count = sum(1 for r in results if 'Error' in r)
        
        # Export to CSV (same as local version)
        if results:
            csv_columns = ['File Name', 'Source Language', 'Extraction Timestamp'] + SIRION_FIELDS + ['Confidence', 'Notes']
            
            with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=csv_columns, extrasaction='ignore')
                writer.writeheader()
                writer.writerows(results)
            
            print(f"\n{'='*80}")
            print(f"‚úÖ Extraction Complete!")
            print(f"{'='*80}")
            print(f"üìä Total Files: {len(blobs)}")
            print(f"‚úÖ Successful: {success_count}")
            print(f"‚ùå Failed: {error_count}")
            print(f"üíæ Output: {output_csv}")
            
            # Summary statistics (same as local version)
            if success_count > 0:
                languages = {}
                confidences = {'high': 0, 'medium': 0, 'low': 0}
                
                for row in results:
                    if 'Source Language' in row:
                        lang = row.get('Source Language', 'unknown')
                        languages[lang] = languages.get(lang, 0) + 1
                    
                    if 'Confidence' in row:
                        conf = row.get('Confidence', 'medium')
                        if conf in confidences:
                            confidences[conf] += 1
                
                print(f"\nüåç Languages Detected:")
                for lang, count in languages.items():
                    print(f"   {lang.upper()}: {count} contracts")
                
                print(f"\nüìà Confidence Distribution:")
                for conf, count in confidences.items():
                    print(f"   {conf.title()}: {count} contracts")
                
                # Calculate cost
                avg_tokens_per_doc = 2000
                total_tokens = success_count * avg_tokens_per_doc
                cost_per_million = 0.15
                estimated_cost = (total_tokens / 1_000_000) * cost_per_million
                
                print(f"\nüí∞ Cost Estimate:")
                print(f"   Total Tokens: ~{total_tokens:,}")
                print(f"   Estimated Cost: ${estimated_cost:.2f}")
            
            print(f"\nüí° Next Steps:")
            print(f"   1. Open {output_csv} in Excel")
            print(f"   2. Review contracts with 'low' or 'medium' confidence")
            print(f"   3. Verify company names and dates")
            print(f"   4. Import to Sirion.ai CLM platform")
            print(f"\n‚ú® Benefits of Direct Extraction:")
            print(f"   ‚úÖ No translation needed (98% cost savings)")
            print(f"   ‚úÖ Single-step process (60% faster)")
            print(f"   ‚úÖ Original legal terminology preserved")
            print(f"   ‚úÖ Supports .txt, .docx, .pdf formats")
            print(f"   ‚úÖ Direct access from Azure Blob Storage")
            print(f"\n{'='*80}\n")
        
        return output_csv
        
    except Exception as e:
        print(f"‚ùå Error accessing blob storage: {str(e)}")
        print(f"   Container: {container_name}")
        print(f"   Storage Account: {storage_account_name}")
        return None


def process_contracts_to_csv(input_folder, output_csv="sirion_metadata.csv", parallel=True):
    """
    Process all contracts and export to CSV
    Supports .txt, .docx, and .pdf files
    
    Args:
        input_folder: Folder containing original contracts
        output_csv: Output CSV file name
        parallel: Use parallel processing (default: True)
    """
    
    input_path = Path(input_folder)
    if not input_path.exists():
        print(f"‚ùå Folder not found: {input_folder}")
        return None
    
    # Find all contract files
    files = []
    for ext in ['*.txt', '*.docx', '*.pdf']:
        files.extend(list(input_path.glob(ext)))
    
    if not files:
        print(f"‚ö†Ô∏è No contract files found in {input_folder}")
        print(f"   Looking for: .txt, .docx, .pdf files")
        return None
    
    print(f"\n{'='*80}")
    print(f"üåç Direct Metadata Extraction - No Translation Needed!")
    print(f"{'='*80}")
    print(f"üìÅ Input Folder: {input_folder}")
    print(f"üìä Files Found: {len(files)}")
    print(f"üìÑ Formats: .txt, .docx, .pdf (with OCR)")
    print(f"ü§ñ Model: {DEPLOYMENT_NAME}")
    if parallel:
        print(f"‚ö° Parallel Mode: {MAX_WORKERS} concurrent workers")
    else:
        print(f"üê¢ Sequential Mode: One at a time")
    print(f"üíæ Output: {output_csv}")
    print(f"{'='*80}\n")
    
    # Prepare file info for processing
    file_infos = []
    for file_path in files:
        file_infos.append({
            'name': file_path.name,
            'data': str(file_path),  # Pass path as string
            'extension': file_path.suffix.lower()
        })
    
    # Process contracts
    results = []
    progress = ProgressTracker(len(file_infos))
    
    if parallel and len(file_infos) > 1:
        # Parallel processing
        print(f"üöÄ Processing {len(file_infos)} contracts in parallel...\n")
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = {
                executor.submit(process_contract_with_retry, file_info, progress): file_info['name']
                for file_info in file_infos
            }
            
            for future in as_completed(futures):
                results.append(future.result())
    else:
        # Sequential processing
        print(f"Processing {len(file_infos)} contracts sequentially...\n")
        for file_info in file_infos:
            results.append(process_contract_with_retry(file_info, progress))
    
    # Count successes/failures
    success_count = sum(1 for r in results if 'Error' not in r)
    error_count = sum(1 for r in results if 'Error' in r)
    
    # Export to CSV
    if results:
        # Define CSV columns
        csv_columns = ['File Name', 'Source Language', 'Extraction Timestamp'] + SIRION_FIELDS + ['Confidence', 'Notes']
        
        # Write CSV
        with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=csv_columns, extrasaction='ignore')
            writer.writeheader()
            writer.writerows(results)
        
        print(f"\n{'='*80}")
        print(f"‚úÖ Extraction Complete!")
        print(f"{'='*80}")
        print(f"üìä Total Files: {len(files)}")
        print(f"‚úÖ Successful: {success_count}")
        print(f"‚ùå Failed: {error_count}")
        print(f"üíæ Output: {output_csv}")
        
        # Summary statistics
        if success_count > 0:
            languages = {}
            confidences = {'high': 0, 'medium': 0, 'low': 0}
            
            for row in results:
                if 'Source Language' in row:
                    lang = row.get('Source Language', 'unknown')
                    languages[lang] = languages.get(lang, 0) + 1
                
                if 'Confidence' in row:
                    conf = row.get('Confidence', 'medium')
                    if conf in confidences:
                        confidences[conf] += 1
            
            print(f"\nüåç Languages Detected:")
            for lang, count in languages.items():
                print(f"   {lang.upper()}: {count} contracts")
            
            print(f"\nüìà Confidence Distribution:")
            for conf, count in confidences.items():
                print(f"   {conf.title()}: {count} contracts")
            
            # Calculate cost
            avg_tokens_per_doc = 2000
            total_tokens = success_count * avg_tokens_per_doc
            cost_per_million = 0.15  # GPT-4o-mini input cost
            estimated_cost = (total_tokens / 1_000_000) * cost_per_million
            
            print(f"\nüí∞ Cost Estimate:")
            print(f"   Total Tokens: ~{total_tokens:,}")
            print(f"   Estimated Cost: ${estimated_cost:.2f}")
        
        print(f"\nüí° Next Steps:")
        print(f"   1. Open {output_csv} in Excel")
        print(f"   2. Review contracts with 'low' or 'medium' confidence")
        print(f"   3. Verify company names and dates")
        print(f"   4. Import to Sirion.ai CLM platform")
        print(f"\n‚ú® Benefits of Direct Extraction:")
        print(f"   ‚úÖ No translation needed (98% cost savings)")
        print(f"   ‚úÖ Single-step process (60% faster)")
        print(f"   ‚úÖ Original legal terminology preserved")
        print(f"   ‚úÖ Supports .txt, .docx, .pdf formats")
        print(f"\n{'='*80}\n")
    
    return output_csv


def process_single_contract(file_path):
    """Test extraction on a single contract"""
    
    path = Path(file_path)
    if not path.exists():
        print(f"‚ùå File not found: {file_path}")
        return None
    
    print(f"\nüîç Testing Direct Metadata Extraction")
    print(f"üìÑ File: {path.name}\n")
    
    # Read contract file (handles .txt, .docx, .pdf)
    contract_text, read_error = read_contract_file(path)
    
    if not contract_text:
        print(f"‚ùå Failed to read file: {read_error}")
        return None
    
    print(f"üìù Contract Length: {len(contract_text)} characters")
    print(f"ü§ñ Processing with {DEPLOYMENT_NAME}...\n")
    
    metadata, error = extract_metadata_direct(contract_text, path.name)
    
    if metadata:
        print("‚úÖ Extraction Successful!\n")
        print(json.dumps(metadata, indent=2))
        
        # Save to JSON
        output_json = path.stem + "_metadata.json"
        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2)
        print(f"\nüíæ Saved to: {output_json}")
        
        return metadata
    else:
        print(f"‚ùå Extraction Failed: {error}")
        return None


if __name__ == "__main__":
    import sys
    
    print("\n" + "="*80)
    print("üåç Circle K Contract Metadata Extractor - Direct Extraction Mode")
    print("="*80)
    print("üìã Extracts metadata from original language contracts (no translation)")
    print("ü§ñ Uses GPT-4o-mini multilingual capabilities")
    print("üìÑ Supports: .txt, .docx, .pdf (with OCR)")
    print("üíæ Outputs to CSV for Sirion.ai import")
    print("üì¶ Source: Azure Blob Storage or Local Files")
    print("="*80 + "\n")
    
    if len(sys.argv) > 1:
        # Single file mode
        file_path = sys.argv[1]
        process_single_contract(file_path)
    else:
        # Ask user for source type
        print("Select source:")
        print("  1. Azure Blob Storage (recommended)")
        print("  2. Local folder")
        source_choice = input("\nEnter choice (1 or 2, default=1): ").strip()
        
        if source_choice == "2":
            # Local folder mode
            input_folder = input("Enter folder path with contracts (or press Enter for 'sample_contracts'): ").strip()
            if not input_folder:
                input_folder = "sample_contracts"
            
            output_csv = input("Enter output CSV name (or press Enter for 'sirion_metadata.csv'): ").strip()
            if not output_csv:
                output_csv = "sirion_metadata.csv"
            
            process_contracts_to_csv(input_folder, output_csv)
        else:
            # Azure Blob Storage mode (default)
            container_name = input("Enter Azure Blob Storage container name (or press Enter for 'documents'): ").strip()
            if not container_name:
                container_name = "documents"
            
            output_csv = input("Enter output CSV name (or press Enter for 'sirion_metadata.csv'): ").strip()
            if not output_csv:
                output_csv = "sirion_metadata.csv"
            
            process_contracts_from_blob(container_name, output_csv)
